{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Right Way to Over-Sample in Model Training\n",
    "\n",
    "Imbalanced datasets are everywhere. Amazon wants to classify fake reviews, banks want to predict fraudulent credit card charges, and, as of this November, Facebook researchers are probably wondering if they can predict which news articles are fake.\n",
    "\n",
    "In each of these cases, only a small fraction of observations are actually positives. I'd guess that only 1 in 10,000 credit card charges are fradulent, at most. Because of this, oversampling the minority class observations has become a common technique to improve the quality of models. By oversampling, models may be able to better learn patterns that differentiate classes. However, this post isn't about why this can improve modeling.\n",
    "\n",
    "Instead, I'm going to detail the _**timing**_ of oversampling can affect the generalization ability of a model. Since the a primary goal of model validation is to estimate how it will perform on unseen data (in production), oversampling correctly is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data\n",
    "\n",
    "I'm going to try to predict whether someone will default on or a creditor will have to charge off a loan, using data from Lending Club. I'll start by importing some packages and loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                       1077501\n",
       "member_id                                                                1296599\n",
       "loan_amnt                                                                   5000\n",
       "funded_amnt                                                                 5000\n",
       "funded_amnt_inv                                                             4975\n",
       "term                                                                   36 months\n",
       "int_rate                                                                   10.65\n",
       "installment                                                               162.87\n",
       "grade                                                                          B\n",
       "sub_grade                                                                     B2\n",
       "emp_title                                                                    NaN\n",
       "emp_length                                                             10+ years\n",
       "home_ownership                                                              RENT\n",
       "annual_inc                                                                 24000\n",
       "is_inc_v                                                                Verified\n",
       "issue_d                                                          20111201T000000\n",
       "loan_status                                                           Fully Paid\n",
       "pymnt_plan                                                                     n\n",
       "url                            https://www.lendingclub.com/browse/loanDetail....\n",
       "desc                             Borrower added on 12/22/11 > I need to upgra...\n",
       "purpose                                                              credit_card\n",
       "title                                                                   Computer\n",
       "zip_code                                                                   860xx\n",
       "addr_state                                                                    AZ\n",
       "dti                                                                        27.65\n",
       "delinq_2yrs                                                                    0\n",
       "earliest_cr_line                                                 19850101T000000\n",
       "inq_last_6mths                                                                 1\n",
       "mths_since_last_delinq                                                       NaN\n",
       "mths_since_last_record                                                       NaN\n",
       "                                                     ...                        \n",
       "total_pymnt                                                              5861.07\n",
       "total_pymnt_inv                                                          5831.78\n",
       "total_rec_prncp                                                             5000\n",
       "total_rec_int                                                             861.07\n",
       "total_rec_late_fee                                                             0\n",
       "recoveries                                                                     0\n",
       "collection_recovery_fee                                                        0\n",
       "last_pymnt_d                                                     20150101T000000\n",
       "last_pymnt_amnt                                                           171.62\n",
       "next_pymnt_d                                                                 NaN\n",
       "last_credit_pull_d                                               20150101T000000\n",
       "collections_12_mths_ex_med                                                     0\n",
       "mths_since_last_major_derog                                                  NaN\n",
       "policy_code                                                                    1\n",
       "not_compliant                                                                  0\n",
       "status                                                                Fully Paid\n",
       "inactive_loans                                                                 1\n",
       "bad_loans                                                                      0\n",
       "emp_length_num                                                                11\n",
       "grade_num                                                                      5\n",
       "sub_grade_num                                                                0.4\n",
       "delinq_2yrs_zero                                                               1\n",
       "pub_rec_zero                                                                   1\n",
       "collections_12_mths_zero                                                       1\n",
       "short_emp                                                                      0\n",
       "payment_inc_ratio                                                         8.1435\n",
       "final_d                                                          20141201T000000\n",
       "last_delinq_none                                                               1\n",
       "last_record_none                                                               1\n",
       "last_major_derog_none                                                          1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans = pd.read_csv('../lending-club-data.csv.zip')\n",
    "loans.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of cool person and loan-specific information in this dataset. The target variable is `bad_loans`, which is 1 if the loan was charged off or the lessee defaulted, and 0 otherwise. I know this data is imbalanced, how imbalanced is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    99457\n",
       "1    23150\n",
       "Name: bad_loans, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans.bad_loans.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charge offs occurred or people defaulted on about 19% of loans, so there's some imbalance in the data but it's not terrible. I'll remove a few observations with missing values for a relevant feature and then pick a handful of features to use in a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans = loans[~loans.payment_inc_ratio.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_variables = ['grade', 'home_ownership','emp_length_num', 'sub_grade','short_emp',\n",
    "            'dti', 'term', 'purpose', 'int_rate', 'last_delinq_none', 'last_major_derog_none',\n",
    "            'revol_util', 'total_rec_late_fee', 'payment_inc_ratio', 'bad_loans']\n",
    "\n",
    "loans_data_relevent = loans[model_variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I need to one-hot encode the categorical features as binary variables to use them in sklearn' random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans_relevant_enconded = pd.get_dummies(loans_data_relevent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data prepared, I can create a training dataset and a test dataset. I'll use the training dataset to build and validate the model, and treat the test dataset as the \"unseen\" new data I'd see if the model were in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_features, test_features, \\\n",
    "training_target, test_target, = train_test_split(loans_relevant_enconded.drop(['bad_loans'], axis=1),\n",
    "                                               loans_relevant_enconded['bad_loans'],\n",
    "                                               test_size = .1,\n",
    "                                               random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wrong Way to Oversample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With my training data created, I'll upsample the bad loans using the SMOTE algorithm (Synthetic Minority Oversampling Technique). At a high level, SMOTE creates synthetic observations of the minority class (bad loans) by:\n",
    "\n",
    "1. Finding the k-nearest-neighbors for minority class observations (finding similar observations)\n",
    "2. Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observation.\n",
    "\n",
    "After upsampling with to a class ratio of 1.0, I should have a balanced dataset. There's no need (and often it's not smart) to balance the classes, but it magnifies the issue caused by incorrectly timed oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "x_res, y_res = sm.fit_sample(training_features, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    89493\n",
      "1    20849\n",
      "Name: bad_loans, dtype: int64 [89493 89493]\n"
     ]
    }
   ],
   "source": [
    "print training_target.value_counts(), np.bincount(y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After upsampling, I'll split the data into separate training and validation sets and build a random forest model to classify the bad loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_res, x_val_res, y_train_res, y_val_res = train_test_split(x_res,\n",
    "                                                    y_res,\n",
    "                                                    test_size = .1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88468629532376108"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=25, random_state=12)\n",
    "clf_rf.fit(x_train_res, y_train_res)\n",
    "clf_rf.score(x_val_res, y_val_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "88% accuracy looks good, but I'm not just interested in accuracy. I also want to know how well I can specifically classify bad loans, since they're more important. In statistics, this is called [recall](https://en.wikipedia.org/wiki/Sensitivity_and_specificity), and it's the number of correctly predicted \"positives\" divided by the total number of \"positives\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81192097332291546"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_val_res, clf_rf.predict(x_val_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "81% recall. That means the model correctly identified 81% of the total bad loans. That's pretty great. But is this actually representative of how the model will perform? To find out, I'll calculate the accuracy and recall for the model on the test dataset I created initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.801973737868\n",
      "0.129943502825\n"
     ]
    }
   ],
   "source": [
    "print clf_rf.score(test_features, test_target)\n",
    "print recall_score(test_target, clf_rf.predict(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 80% accuracy and 13% recall on the test data. That's a **huge** difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Happened?\n",
    "\n",
    "When the model is in production, it's predicting on unseen data. The entire point of using a validation set is to estimate how the model will generalize to that new data.\n",
    "\n",
    "By oversampling before splitting into training and validation datasets, I \"bleed\" information from the validation set into the training of the model.\n",
    "\n",
    "To see how this works, think about the case of simple oversampling (where I just duplicate observations). If I upsample a dataset before splitting it into a train and validation set, I could end up with the same observation in both datasets. As a result, the model will be able to perfectly predict the value for those observations when predicting on the validation set, inflating the accuracy and recall.\n",
    "\n",
    "When upsampling using SMOTE, I don't create duplicate observations. However, because the SMOTE algorithm uses the nearest neighbors of observations to create synthetic data, it still \"bleed\" information. If the nearest neighbors of minority class observations in the training set end up in the validation set, their information is partially reflected by the synthetic data in the training set. Since I'm splitting the data randomly, we'd expect to have this happen. As a result, the model will be better able to predict validation set values than on completely new data.\n",
    "\n",
    "When I predict on the unseen test data, though, the \"false boost\" disappears, and I get the true generalization results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Right Way to Oversample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so I've gone through the wrong way to oversample. Now I'll go through the right way: oversampling on only the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(training_features, training_target,\n",
    "                                                  test_size = .1,\n",
    "                                                  random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "x_train_res, y_train_res = sm.fit_sample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By oversampling only on the training data, none of the information in the validation data is being used to create synthetic observations. So these results should be generalizable. Let's see if that's true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=25, n_jobs=1, oob_score=False, random_state=12,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=25, random_state=12)\n",
    "clf_rf.fit(x_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results\n",
      "0.800362483009\n",
      "0.138195777351\n",
      "\n",
      "Test Results\n",
      "0.803278688525\n",
      "0.142546718818\n"
     ]
    }
   ],
   "source": [
    "print 'Validation Results'\n",
    "print clf_rf.score(x_val, y_val)\n",
    "print recall_score(y_val, clf_rf.predict(x_val))\n",
    "print '\\nTest Results'\n",
    "print clf_rf.score(test_features, test_target)\n",
    "print recall_score(test_target, clf_rf.predict(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation results closely match the \"unseen\" test data results, which is exactly what I would want to see after putting a model into production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling is well-covered way to potentially improve models trained on imbalanced data. But it's important to remember that oversampling incorrectly can lead to thinking a model will generalize better than it actually does. Random forests are great because they don't overfit (see [Brieman 2001](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) for a proof), but poor sampling practices can still lead to false conclusions about the quality of a model.\n",
    "\n",
    "If the decision to put a model into production is based on how it performs on a validation set, it's critical that oversampling is done correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
